## Acknowledgments

We would like to thank [Blake Richards](http://linclab.org/blake-richards/), [Kory Mathewson](http://korymathewson.com/), [Kyle McDonald](http://www.kylemcdonald.net), [Kai Arulkumaran](http://kaixhin.com), [Ankur Handa](https://ankurhanda.github.io), [Denny Britz](http://www.wildml.com/), [Elwin Ha](http://elwinha.com) and [Natasha Jaques](https://www.media.mit.edu/people/jaquesn/overview/) for their thoughtful feedback on this article, and for offering their valuable perspectives and insights from their areas of expertise.

The interative demos in this article were all built using [p5.js](https://p5js.org/). Deploying all of these machine learning models in a web browser was made possible with [deeplearn.js](https://deeplearnjs.org/), a hardware-accelerated machine learning framework for the browser, developed by the [People+AI Research Initiative](https://ai.google/pair) (PAIR) team at Google. A special thanks goes to Nikhil Thorat and Daniel Smilkov for their support.

We would like to thank Chris Olah and the rest of the Distill editorial team for their valuable feedback and generous editorial support, in addition to supporting the use of their [distill.pub](https://distill.pub) technology.

We would to extend our thanks to Alex Graves, Douglas Eck, Mike Schuster, Rajat Monga, Vincent Vanhoucke, Jeff Dean and the Google Brain team for helpful feedback and for encouraging us to explore this area of research.

Any errors here are our own and do not reflect opinions of our proofreaders and colleagues. If you see mistakes or want to suggest changes, feel free to contribute feedback by participating in the discussion [forum](https://github.com/worldmodels/worldmodels.github.io/issues) for this article.

The experiments in this article were performed on both a P100 GPU and a 64-core CPU Ubuntu Linux virtual machine provided by [Google Cloud Platform](https://cloud.google.com/), using [TensorFlow](https://www.tensorflow.org/) and [OpenAI Gym](https://github.com/openai/gym).

<h3 id="citation">Citation</h3>

For attribution in academic contexts, please cite this work as

<pre class="citation short">Ha and Schmidhuber, "World Models", 2018. https://doi.org/10.5281/zenodo.1207631</pre>

BibTeX citation

<pre class="citation long">@article{Ha2018WorldModels,
  author = {Ha, D. and Schmidhuber, J.},
  title  = {World Models},
  eprint = {arXiv:1803.10122},
  doi    = {10.5281/zenodo.1207631},
  url    = {https://worldmodels.github.io},
  year   = {2018}
}</pre>

### Open Source Code

The code to reproduce experiments in this work, as well as IPython notebooks for training and visualizing VAE and MDN-RNN models will be made available at a later date.

### Reuse

Diagrams and text are licensed under Creative Commons Attribution [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) with the [source available on GitHub](https://github.com/worldmodels/worldmodels.github.io), unless noted otherwise. The figures that have been reused from other sources donâ€™t fall under this license and can be recognized by the citations in their caption.

<h2 id="appendix">Appendix</h2>

In this section we will describe in more details the models and training methods used in this work.

### Variational Autoencoder

We trained a Convolutional Variational Autoencoder (ConvVAE) model as the V Model of our agent. Unlike vanilla autoencoders, enforcing a Gaussian prior over the latent vector $z$ also limits the amount its information capacity for compressing each frame, but this Gaussian prior also makes the world model more robust to unrealistic $z$ vectors generated by the M Model. As the environment may give us observations as high dimensional pixel images, we first resize each image to 64x64 pixels before and use this resized image as the V Model's observation. Each pixel is stored as three floating point values between 0 and 1 to represent each of the RGB channels. The ConvVAE takes in this 64x64x3 input tensor and passes this data through 4 convolutional layers to *encode* it into low dimension vectors $\mu$ and $\sigma$, each of size $N_z$. The latent vector $z$ is sampled from the Gaussian prior $N(\mu, \sigma I)$. In the <dt-cite key="carracing_v0">Car Racing task</dt-cite>, $N_z$ is 32 while for the Doom task $N_z$ is 64. The latent vector $z$ is passed through 4 of *deconvolution* layers used to *decode* and reconstruct the image.

In the following diagram, we describe the shape of our tensor at each layer of the ConvVAE and also describe the details of each layer:

<div style="text-align: center;">
<img src="assets/conv_vae_label.svg" style="display: block; margin: auto; width: 50%;"/>
<figcaption>Convolutional Variational Autoencoder</figcaption>
</div>

Each convolution and deconvolution layer uses a stride of 2. The layers are indicated in the diagram in *Italics* as *Activation-type Output Channels x Filter Size*. All convolutional and deconvolutional layers use relu activations except for the output layer as we need the output to be between 0 and 1. We trained the model for 1 epoch over the data collected from a random policy, using $L^2$ distance between the input image and the reconstruction to quantify the reconstruction loss we optimize for, in addition to the KL loss.

### Recurrent Neural Network

For the M Model, we use an <dt-cite key="lstm">LSTM</dt-cite> recurrent neural network combined with a Mixture Density Network<dt-cite key="bishop"></dt-cite><dt-cite key="mdntf"></dt-cite> as the output layer. We use this network to model the probability distribution of the next $z$ in the next time step as a Mixture of Gaussian distribution. This approach is very similar to Graves' <dt-cite key="graves_rnn">Generating Sequences with RNNs</dt-cite> in the Unconditional Handwriting Generation section and also the decoder-only section of <dt-cite key="sketchrnn">Sketch-RNN</dt-cite>. The only difference in the approach used is that we did not model the correlation parameter between each element of $z$, and instead had the MDN-RNN output a diagonal covariance matrix of a factored Gaussian distribution.

<div style="text-align: center;">
<img src="assets/mdn_rnn.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>MDN-RNN<dt-cite key="sketchrnn"></dt-cite></figcaption>
</div>

Unlike the handwriting and sketch generation works, rather than using the MDN-RNN to model the pdf of the next pen stroke, we model instead the pdf of the next latent vector $z$. We would sample from this pdf at each time step to generate the hallucinated environments. In the Doom task, we also also use the MDN-RNN to predict the probability of whether the agent has died in this frame. If that probability is above 50%, then we set <code>done</code> to be <code>True</code> in the virtual dream environment. Given that death is a low probability event at each time step, we find the cutoff approach to more stable compared to sampling from the Bernoulli distribution.

The MDN-RNNs were trained for 20 epochs on the data collected from a random policy agent. In the Car Racing task, the LSTM used 256 hidden units, while the Doom task used 512 hidden units. In both tasks, we used 5 Gaussian mixtures and did not model the correlation $\rho$ parameter, hence $z$ is sampled from a factored mixture of Gaussian distribution.

When training the MDN-RNN using teacher forcing from the recorded data, we store a pre-computed set of $\mu$ and $\sigma$ for each of the frames, and sample an input $z \sim N(\mu, \sigma)$ each time we construct a training batch, to prevent overfitting our MDN-RNN to a specific sampled $z$. 

### Controller

For both environments, we applied $\tanh$ nonlinearities to clip and bound the action space to the appropriate ranges. For instance, in the Car Racing task, the steering wheel has a range from -1 to 1, the acceleration pedal from 0 to 1, and the brakes from 0 to 1. In the Doom environment, we converted the discrete actions into a continuous action space between -1 to 1, and divided this range into thirds to indicate whether the agent is moving left, staying where it is, or moving to the right. We would give the C model a feature vector as its input, consisting of $z$ and the hidden state of the MDN-RNN. In the Car Racing task, this hidden state is the output vector $h$ of the LSTM, while for the Doom task it is both the cell vector $c$ and the output vector $h$ of the LSTM.

### Evolution Strategies

We used <dt-cite key="cmaes">Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)</dt-cite>, an <dt-cite key="visuales">Evolution Strategy</dt-cite> to evolve the weights for our C model. Following the approach described in <dt-cite key="stablees">Evolving Stable Strategies</dt-cite>, we used a population size of 64, and had each agent perform the task 16 times with different initial random seeds. The fitness value for the agent is the *average cumulative reward* of the 16 random rollouts. The diagram below charts the best performer, worst performer, and mean fitness of the population of 64 agents at each generation:

<div style="text-align: center;">
<img src="https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/assets/carracing.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>Training of <dt-cite key="carracing_v0">CarRacing-v0</dt-cite></figcaption>
</div>

Since the requirement of this environment is to have an agent achieve an average score above 900 over 100 random rollouts, we took the best performing agent at the end of every 25 generations, and tested that agent over 1024 random rollout scenarios to record this average on the red line. After 1800 generations, an agent was able to achieve an average score of 900.46 over 1024 random rollouts. We used 1024 random rollouts rather than 100 because each process of the 64 core machine had been configured to run 16 times already, effectively using a full generation of compute after every 25 generations to evaluate the best agent 1024 times. Below, we plot the results of same agent evaluated over 100 rollouts:

<div style="text-align: center;">
<img src="assets/carracing_histogram.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>Histogram of cumulative rewards. Average score is 906 Â± 21.</figcaption>
</div>

We also experimented with an agent that has access to only the $z$ vector from the VAE, and not letting it see the RNN's hidden states. We tried 2 variations, where in the first variation, the C model mapped $z$ directly to the action space $a$. In second variation, we attempted to add a hidden layer with 40 $tanh$ activations between $z$ and $a$, increasing the number of model parameters of the C model to 1443, making it more comparable with the original setup.

<div style="text-align: center;">
<img src="assets/carracing_histogram_z.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>When agent sees only <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">â€‹</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">â€‹</span></span>â€‹</span></span></span></span></span></span>, average score is 632 Â± 251.</figcaption>
</div>

<div style="text-align: center;">
<img src="assets/carracing_histogram_z_hidden.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>When agent sees only <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">â€‹</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">â€‹</span></span>â€‹</span></span></span></span></span></span>, with a hidden layer, average score is 788 Â± 141.</figcaption>
</div>

### DoomRNN

We conducted a similar experiment on the hallucinated Doom environment we called *DoomRNN*. Please note that we have not actually attempted to train our agent on the actual <dt-cite key="vizdoom">VizDoom</dt-cite> environment, and had only used VizDoom for the purpose of collecting training data using a random policy. *DoomRNN* is more computationally efficient compared to VizDoom as it only operates in latent space without the need to render a screenshot at each time step, and does not require running the actual Doom game engine.

<div style="text-align: center;">
<img src="https://storage.googleapis.com/quickdraw-models/sketchRNN/world_models/assets/doomrnn.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>Training of DoomRNN</figcaption>
</div>

In the virtual DoomRNN environment we constructed, we increased the temperature slightly and used $\tau=1.15$ to make the agent learn in a more challenging environment. The best agent managed to obtain an average score of 959 over 1024 random rollouts (the highest score of the red line in the diagram). This same agent achieved an average score of 1092 $\pm$ 556 over 100 random rollouts when deployed to the actual environment <dt-cite key="takecover">DoomTakeCover-v0</dt-cite>.

<div style="text-align: center;">
<img src="assets/doomtakecover_histogram.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>Histogram of time steps survived in the actual environment over 100 consecutive trials. Average score is 1092 Â± 556.</figcaption>
</div>